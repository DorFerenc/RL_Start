{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "661bcff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: gymnasium==1.0.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.0.0)\n",
      "Requirement already satisfied: moviepy==1.0.3 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.0.0) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.0.0) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.0.0) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.0.0) (0.0.4)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (2.37.2)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (2.28.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (0.1.12)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (10.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0,>=4.11.2->moviepy==1.0.3) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\dorfe\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium==1.0.0 moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f311cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from IPython.display import Video\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "# from google.colab import files\n",
    "import imageio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd46a6b",
   "metadata": {},
   "source": [
    "# Assignment 1: Policy Iteration in the Repeated Prisoner's Dilemma\n",
    "--- \n",
    "* Authors: Sara, Dor\n",
    "* IDs: \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2f8d0",
   "metadata": {},
   "source": [
    "### Step 1: install and import\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9099fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install gymnasium==1.0.0 moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8575c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "from enum import Enum\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b9b9ac",
   "metadata": {},
   "source": [
    "### Step 2: MDP Components - Constants and Definitions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f1f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(Enum):\n",
    "    \"\"\"\n",
    "    Action Space A = {COOPERATE, DEFECT}\n",
    "\n",
    "    In RL terms: These are the actions available to the agent at each timestep.\n",
    "    \"\"\"\n",
    "    COOPERATE = 0  # a = 0\n",
    "    DEFECT = 1     # a = 1\n",
    "\n",
    "\n",
    "class OpponentStrategy(Enum):\n",
    "    \"\"\"\n",
    "    The opponent's policy (fixed, not learned).\n",
    "\n",
    "    In RL terms: The opponent is part of the ENVIRONMENT, not the agent.\n",
    "    The opponent's policy determines part of the transition dynamics P(s'|s,a).\n",
    "    \"\"\"\n",
    "    ALL_C = \"all_c\"                  # π_opp(s) = COOPERATE for all s\n",
    "    ALL_D = \"all_d\"                  # π_opp(s) = DEFECT for all s\n",
    "    TFT = \"tft\"                      # π_opp(s) = agent's last action\n",
    "    IMPERFECT_TFT = \"imperfect_tft\"  # π_opp(s) = agent's last action + noise\n",
    "\n",
    "\n",
    "class MemoryType(Enum):\n",
    "    \"\"\"\n",
    "    Determines the State Space S.\n",
    "\n",
    "    Memory-1: |S| = 4 states\n",
    "    Memory-2: |S| = 16 states\n",
    "    \"\"\"\n",
    "    MEMORY_1 = 1\n",
    "    MEMORY_2 = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc55738",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Reward Function R(s, a, s') - but since opponent action determines s',\n",
    "we can express this as R(my_action, opponent_action)\n",
    "\n",
    "The immediate reward depends on the joint action (my_action, opponent_action)\n",
    "This is the REWARD FUNCTION of our MDP\n",
    "\n",
    "| You \\ Opponent | Cooperate (C) | Defect (D) |\n",
    "|----------------|---------------|------------|\n",
    "| Cooperate (C)  | R = 3         | S = 0      |\n",
    "| Defect (D)     | T = 5         | P = 1      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "336313ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "REWARD_FUNCTION = {\n",
    "    # R(a_agent, a_opponent) -> immediate reward\n",
    "    (Action.COOPERATE, Action.COOPERATE): 3.0,  # R: Mutual cooperation\n",
    "    (Action.COOPERATE, Action.DEFECT): 0.0,     # S: Sucker's payoff\n",
    "    (Action.DEFECT, Action.COOPERATE): 5.0,     # T: Temptation payoff\n",
    "    (Action.DEFECT, Action.DEFECT): 1.0,        # P: Mutual defection\n",
    "}\n",
    "\n",
    "# Transition noise for Imperfect TFT\n",
    "SLIP_PROBABILITY = 0.1  # ε = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4965c",
   "metadata": {},
   "source": [
    "### Step 3: The MDP Environment\n",
    "---\n",
    "\n",
    "Repeated Prisoner's Dilemma as a Markov Decision Process (MDP).\n",
    "    \n",
    "MDP Components\n",
    "--------------\n",
    "S (State Space):\n",
    "- Memory-1: S = {(a_{t-1}, b_{t-1})} where a=agent action, b=opponent action\n",
    "        |S| = 2 × 2 = 4 states\n",
    "- Memory-2: S = {(a_{t-2}, a_{t-1}, b_{t-2}, b_{t-1})}\n",
    "        |S| = 2 × 2 × 2 × 2 = 16 states\n",
    "\n",
    "A (Action Space):\n",
    "- A = {COOPERATE (0), DEFECT (1)}\n",
    "- |A| = 2 actions\n",
    "\n",
    "P(s'|s, a) (Transition Model):\n",
    "- Depends on opponent's policy (part of environment dynamics).\n",
    "- Deterministic opponents (ALL_C, ALL_D, TFT): P(s'|s,a) ∈ {0, 1}\n",
    "- Stochastic opponent (IMPERFECT_TFT): P(s'|s,a) ∈ {0.1, 0.9}\n",
    "\n",
    "R(s, a) (Reward Function):\n",
    "- Expected immediate reward when taking action a in state s.\n",
    "- For deterministic opponents: R(s,a) = r (the payoff)\n",
    "- For stochastic opponents: R(s,a) = Σ P(s'|s,a) × r(s,a,s')\n",
    "\n",
    "γ (Discount Factor):\n",
    "- Specified when running Policy Iteration, not stored in environment.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "opponent_strategy : str\n",
    "- The fixed policy of the opponent: 'all_c', 'all_d', 'tft', 'imperfect_tft'\n",
    "memory : int\n",
    "- Determines state space size: 1 for Memory-1, 2 for Memory-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbf980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrisonersDilemmaMDP(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        opponent_strategy: str = \"tft\",\n",
    "        memory: int = 1\n",
    "    ):\n",
    "        \"\"\"Initialize the MDP.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # =====================================================================\n",
    "        # Store MDP Configuration\n",
    "        # =====================================================================\n",
    "\n",
    "        # Convert to enums\n",
    "        if isinstance(opponent_strategy, str):\n",
    "            self.opponent_strategy = OpponentStrategy(opponent_strategy.lower())\n",
    "        else:\n",
    "            self.opponent_strategy = opponent_strategy\n",
    "\n",
    "        if isinstance(memory, int):\n",
    "            self.memory = MemoryType(memory)\n",
    "        else:\n",
    "            self.memory = memory\n",
    "\n",
    "        # =====================================================================\n",
    "        # Define Action Space A\n",
    "        # =====================================================================\n",
    "        # A = {0: COOPERATE, 1: DEFECT}\n",
    "        # |A| = 2\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.num_actions = 2\n",
    "\n",
    "        # =====================================================================\n",
    "        # Define State Space S\n",
    "        # =====================================================================\n",
    "        if self.memory == MemoryType.MEMORY_1:\n",
    "            # S = {(a_{t-1}, b_{t-1})}\n",
    "            # States: (C,C), (C,D), (D,C), (D,D)\n",
    "            # |S| = 4\n",
    "            self.observation_space = spaces.MultiDiscrete([2, 2])\n",
    "            self.num_states = 4\n",
    "        else:\n",
    "            # S = {(a_{t-2}, a_{t-1}, b_{t-2}, b_{t-1})}\n",
    "            # |S| = 16\n",
    "            self.observation_space = spaces.MultiDiscrete([2, 2, 2, 2])\n",
    "            self.num_states = 16\n",
    "\n",
    "        # =====================================================================\n",
    "        # History Tracking (needed to determine current state)\n",
    "        # =====================================================================\n",
    "        self.agent_history = []\n",
    "        self.opponent_history = []\n",
    "\n",
    "        # Episode tracking\n",
    "        self.timestep = 0\n",
    "        self.cumulative_reward = 0.0\n",
    "\n",
    "    # =========================================================================\n",
    "    # Core MDP Methods\n",
    "    # =========================================================================\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None\n",
    "    ) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"\n",
    "        Reset the MDP to initial state s_0.\n",
    "\n",
    "        Initial State Convention:\n",
    "            We assume \"phantom\" cooperation before t=0.\n",
    "            - Memory-1: s_0 = (C, C)\n",
    "            - Memory-2: s_0 = (C, C, C, C)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : np.ndarray\n",
    "            Initial state s_0\n",
    "        info : dict\n",
    "            Additional information\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset tracking\n",
    "        self.timestep = 0\n",
    "        self.cumulative_reward = 0.0\n",
    "\n",
    "        # Initialize history (phantom cooperation)\n",
    "        if self.memory == MemoryType.MEMORY_1:\n",
    "            self.agent_history = [Action.COOPERATE.value]\n",
    "            self.opponent_history = [Action.COOPERATE.value]\n",
    "            initial_state = np.array([0, 0], dtype=np.int64)\n",
    "        else:\n",
    "            self.agent_history = [Action.COOPERATE.value, Action.COOPERATE.value]\n",
    "            self.opponent_history = [Action.COOPERATE.value, Action.COOPERATE.value]\n",
    "            initial_state = np.array([0, 0, 0, 0], dtype=np.int64)\n",
    "\n",
    "        return initial_state, {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
    "        \"\"\"\n",
    "        Execute one step in the MDP.\n",
    "\n",
    "        Given current state s and action a:\n",
    "            1. Environment samples s' ~ P(s'|s,a)\n",
    "            2. Agent receives reward r = R(s,a) or R(s,a,s')\n",
    "            3. State transitions: s ← s'\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            Agent's action a ∈ A = {0, 1}\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        next_state : np.ndarray\n",
    "            The new state s'\n",
    "        reward : float\n",
    "            Immediate reward r\n",
    "        terminated : bool\n",
    "            Whether episode ended (False for infinite horizon)\n",
    "        truncated : bool\n",
    "            Whether episode was cut off (False)\n",
    "        info : dict\n",
    "            Debugging information\n",
    "        \"\"\"\n",
    "        agent_action = Action(action)\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 1: Environment determines opponent action based on P(s'|s,a)\n",
    "        # =====================================================================\n",
    "        opponent_action = self._get_opponent_action()\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 2: Compute immediate reward R(s, a)\n",
    "        # =====================================================================\n",
    "        reward = self._get_reward(agent_action, opponent_action)\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 3: Update history and compute next state s'\n",
    "        # =====================================================================\n",
    "        self.agent_history.append(agent_action.value)\n",
    "        self.opponent_history.append(opponent_action.value)\n",
    "\n",
    "        next_state = self._get_current_state()\n",
    "\n",
    "        # =====================================================================\n",
    "        # Step 4: Update statistics\n",
    "        # =====================================================================\n",
    "        self.timestep += 1\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        # Infinite horizon MDP - never terminates\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        info = {\n",
    "            \"agent_action\": agent_action.name,\n",
    "            \"opponent_action\": opponent_action.name,\n",
    "            \"timestep\": self.timestep,\n",
    "            \"cumulative_reward\": self.cumulative_reward\n",
    "        }\n",
    "\n",
    "        return next_state, reward, terminated, truncated, info\n",
    "\n",
    "    # =========================================================================\n",
    "    # Transition Model P(s'|s, a)\n",
    "    # =========================================================================\n",
    "\n",
    "    def _get_opponent_action(self) -> Action:\n",
    "        \"\"\"\n",
    "        Sample opponent's action based on their fixed policy.\n",
    "\n",
    "        This is part of the Transition Model P(s'|s,a).\n",
    "        The opponent's policy determines how the environment responds\n",
    "        to the agent's action.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Action\n",
    "            The opponent's action this timestep\n",
    "        \"\"\"\n",
    "        if self.opponent_strategy == OpponentStrategy.ALL_C:\n",
    "            # Deterministic: π_opp(s) = C always\n",
    "            # P(opponent=C | s, a) = 1.0\n",
    "            return Action.COOPERATE\n",
    "\n",
    "        elif self.opponent_strategy == OpponentStrategy.ALL_D:\n",
    "            # Deterministic: π_opp(s) = D always\n",
    "            # P(opponent=D | s, a) = 1.0\n",
    "            return Action.DEFECT\n",
    "\n",
    "        elif self.opponent_strategy == OpponentStrategy.TFT:\n",
    "            # Deterministic: π_opp(s) = agent's last action\n",
    "            # P(opponent=a_{t-1} | s, a) = 1.0\n",
    "            last_agent_action = self.agent_history[-1]\n",
    "            return Action(last_agent_action)\n",
    "\n",
    "        elif self.opponent_strategy == OpponentStrategy.IMPERFECT_TFT:\n",
    "            # Stochastic: opponent tries to copy but slips with prob ε\n",
    "            # P(opponent=a_{t-1} | s, a) = 0.9\n",
    "            # P(opponent≠a_{t-1} | s, a) = 0.1\n",
    "            intended_action = Action(self.agent_history[-1])\n",
    "\n",
    "            if random.random() < SLIP_PROBABILITY:\n",
    "                # Slip: do opposite with probability ε = 0.1\n",
    "                return Action(1 - intended_action.value)\n",
    "            else:\n",
    "                # No slip: do intended with probability 1-ε = 0.9\n",
    "                return intended_action\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown opponent strategy: {self.opponent_strategy}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # Reward Function R(s, a)\n",
    "    # =========================================================================\n",
    "\n",
    "    def _get_reward(self, agent_action: Action, opponent_action: Action) -> float:\n",
    "        \"\"\"\n",
    "        Compute immediate reward R(s, a).\n",
    "\n",
    "        In this MDP, the reward depends on the joint action.\n",
    "        Since opponent's action is determined by the transition model,\n",
    "        R(s, a) is well-defined.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        agent_action : Action\n",
    "            Agent's action a\n",
    "        opponent_action : Action\n",
    "            Opponent's action (sampled from environment dynamics)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Immediate reward r\n",
    "        \"\"\"\n",
    "        return REWARD_FUNCTION[(agent_action, opponent_action)]\n",
    "\n",
    "    # =========================================================================\n",
    "    # State Representation\n",
    "    # =========================================================================\n",
    "\n",
    "    def _get_current_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Construct current state s from history.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Current state observation\n",
    "        \"\"\"\n",
    "        if self.memory == MemoryType.MEMORY_1:\n",
    "            # s = (a_{t-1}, b_{t-1})\n",
    "            return np.array([\n",
    "                self.agent_history[-1],\n",
    "                self.opponent_history[-1]\n",
    "            ], dtype=np.int64)\n",
    "        else:\n",
    "            # s = (a_{t-2}, a_{t-1}, b_{t-2}, b_{t-1})\n",
    "            return np.array([\n",
    "                self.agent_history[-2],\n",
    "                self.agent_history[-1],\n",
    "                self.opponent_history[-2],\n",
    "                self.opponent_history[-1]\n",
    "            ], dtype=np.int64)\n",
    "\n",
    "    # =========================================================================\n",
    "    # Helper Methods for Policy Iteration (Part III)\n",
    "    # =========================================================================\n",
    "\n",
    "    def state_to_index(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Convert state array to unique index in {0, 1, ..., |S|-1}.\n",
    "\n",
    "        Useful for tabular methods like Policy Iteration.\n",
    "\n",
    "        Memory-1 mapping:\n",
    "            (C,C)=0, (C,D)=1, (D,C)=2, (D,D)=3\n",
    "\n",
    "        Memory-2 mapping:\n",
    "            Binary encoding: 0-15\n",
    "        \"\"\"\n",
    "        if self.memory == MemoryType.MEMORY_1:\n",
    "            return state[0] * 2 + state[1]\n",
    "        else:\n",
    "            return state[0] * 8 + state[1] * 4 + state[2] * 2 + state[3]\n",
    "\n",
    "    def index_to_state(self, index: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert state index back to state array.\n",
    "\n",
    "        Inverse of state_to_index.\n",
    "        \"\"\"\n",
    "        if self.memory == MemoryType.MEMORY_1:\n",
    "            return np.array([index // 2, index % 2], dtype=np.int64)\n",
    "        else:\n",
    "            return np.array([\n",
    "                (index >> 3) & 1,\n",
    "                (index >> 2) & 1,\n",
    "                (index >> 1) & 1,\n",
    "                index & 1\n",
    "            ], dtype=np.int64)\n",
    "\n",
    "    def get_all_states(self) -> list:\n",
    "        \"\"\"\n",
    "        Return list of all states in S.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of np.ndarray\n",
    "            All possible states\n",
    "        \"\"\"\n",
    "        return [self.index_to_state(i) for i in range(self.num_states)]\n",
    "\n",
    "    def render(self, mode: str = \"human\"):\n",
    "        \"\"\"Display current MDP state for debugging.\"\"\"\n",
    "        print(f\"\\n===== Timestep t={self.timestep} =====\")\n",
    "        print(f\"Agent history (last 5):    {['C' if a==0 else 'D' for a in self.agent_history[-5:]]}\")\n",
    "        print(f\"Opponent history (last 5): {['C' if a==0 else 'D' for a in self.opponent_history[-5:]]}\")\n",
    "        print(f\"Cumulative reward: {self.cumulative_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f6080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
