{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18ef368",
   "metadata": {},
   "source": [
    "# Brick 1: Constants and Setup\n",
    "Here we define the rules of the game.\n",
    "- **0** means Cooperate.\n",
    "- **1** means Defect.\n",
    "- We also define the **Payoff Matrix** (the points we get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de5e050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting gymnasium==1.1.1\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: moviepy==1.0.3 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.1.1) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.1.1) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.1.1) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gymnasium==1.1.1) (0.0.4)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (2.37.2)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (2.28.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from moviepy==1.0.3) (0.1.12)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (10.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0,>=4.11.2->moviepy==1.0.3) (0.4.6)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 965.4/965.4 kB 6.4 MB/s eta 0:00:00\n",
      "Installing collected packages: gymnasium\n",
      "  Attempting uninstall: gymnasium\n",
      "    Found existing installation: gymnasium 1.0.0\n",
      "    Uninstalling gymnasium-1.0.0:\n",
      "      Successfully uninstalled gymnasium-1.0.0\n",
      "Successfully installed gymnasium-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\dorfe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\dorfe\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium==1.1.1 moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17792b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete! Actions and Rewards defined.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# 1. Define Actions for readability\n",
    "COOPERATE = 0\n",
    "DEFECT = 1\n",
    "\n",
    "# 2. Define Rewards (Points for the Agent)\n",
    "# Format: (Agent Action, Opponent Action): Reward\n",
    "PAYOFF_MATRIX = {\n",
    "    (COOPERATE, COOPERATE): 3, # Reward\n",
    "    (COOPERATE, DEFECT):    0, # Sucker\n",
    "    (DEFECT, COOPERATE):    5, # Temptation\n",
    "    (DEFECT, DEFECT):       1, # Punishment\n",
    "}\n",
    "\n",
    "print(\"Setup Complete! Actions and Rewards defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fd7a9",
   "metadata": {},
   "source": [
    "# Brick 2: Opponent Strategies\n",
    "This function decides the opponent's move based on the history of the game.\n",
    "- **ALL_C**: Always nice.\n",
    "- **ALL_D**: Always mean.\n",
    "- **TFT (Tit-for-Tat)**: Copies your last move.\n",
    "- **Imperfect TFT**: Copies your move 90% of the time, messes up 10% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "321f65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opponent_action(strategy, history):\n",
    "    \"\"\"\n",
    "    Decides the opponent's move.\n",
    "    history: A list of tuples, e.g., [(my_move, their_move), (my_move, their_move)]\n",
    "    \"\"\"\n",
    "\n",
    "    # Strategy 1: Always Cooperate\n",
    "    if strategy == \"ALL_C\":\n",
    "        return COOPERATE\n",
    "\n",
    "    # Strategy 2: Always Defect\n",
    "    if strategy == \"ALL_D\":\n",
    "        return DEFECT\n",
    "\n",
    "    # Strategy 3: Tit-for-Tat (Copy my last move)\n",
    "    if strategy == \"TFT\":\n",
    "        if len(history) == 0:\n",
    "            return COOPERATE # TFT starts nice\n",
    "\n",
    "        # Look at the last round (index -1).\n",
    "        # In history (my_move, opp_move), my_move is at index 0.\n",
    "        my_last_move = history[-1][0]\n",
    "        return my_last_move\n",
    "\n",
    "    # Strategy 4: Imperfect Tit-for-Tat (10% chance of error)\n",
    "    if strategy == \"IMPERFECT_TFT\":\n",
    "        if len(history) == 0:\n",
    "            return COOPERATE # Start nice\n",
    "\n",
    "        # Calculate what standard TFT would do\n",
    "        my_last_move = history[-1][0]\n",
    "        intended_action = my_last_move\n",
    "\n",
    "        # 10% chance to flip the action (Slip)\n",
    "        if random.random() < 0.10:\n",
    "            # Return the opposite (0 becomes 1, 1 becomes 0)\n",
    "            return 1 - intended_action\n",
    "        else:\n",
    "            return intended_action\n",
    "\n",
    "    return COOPERATE # Default fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b48f7",
   "metadata": {},
   "source": [
    "# Brick 3: The Game Environment (The Class)\n",
    "This class puts everything together.\n",
    "- **__init__**: Sets up the game options (Opponent type, Memory length).\n",
    "- **step**: Plays one round (Agent moves -> Opponent moves -> Calculate Score -> Update History).\n",
    "- **reset**: Wipes the memory clean to start a new game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02b0a4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Class Defined Successfully!\n"
     ]
    }
   ],
   "source": [
    "class PrisonerDilemmaEnv(gym.Env):\n",
    "    def __init__(self, opponent_strategy, memory_length=1):\n",
    "        super().__init__()\n",
    "        self.opponent_strategy = opponent_strategy\n",
    "        self.memory_length = memory_length\n",
    "\n",
    "        # Define Action Space (0 or 1)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "        # Internal memory to track the game history\n",
    "        self.history = []\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Resets the game to the starting state.\n",
    "        The assignment says to assume everyone cooperated before the game started.\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        self.history = [] # Clear history\n",
    "\n",
    "        # Create the \"Pre-game\" history based on memory length\n",
    "        # If memory is 1, we pretend the last round was (C, C)\n",
    "        # If memory is 2, we pretend the last two rounds were (C, C), (C, C)\n",
    "        for _ in range(self.memory_length):\n",
    "            self.history.append((COOPERATE, COOPERATE))\n",
    "\n",
    "        return self._get_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Plays one round of the game.\n",
    "        \"\"\"\n",
    "        # 1. Agent makes a move (passed in as 'action')\n",
    "\n",
    "        # 2. Opponent makes a move (using our helper function from Brick 2)\n",
    "        opp_action = get_opponent_action(self.opponent_strategy, self.history)\n",
    "\n",
    "        # 3. Calculate Reward (using the Matrix from Brick 1)\n",
    "        reward = PAYOFF_MATRIX[(action, opp_action)]\n",
    "\n",
    "        # 4. Update History\n",
    "        # Add the new round to the end\n",
    "        self.history.append((action, opp_action))\n",
    "\n",
    "        # Remove the oldest round so we only keep what we need for memory\n",
    "        if len(self.history) > self.memory_length:\n",
    "            self.history.pop(0)\n",
    "\n",
    "        # 5. Get the new State\n",
    "        state = self._get_state()\n",
    "\n",
    "        # In this specific assignment, the game repeats indefinitely (no \"Game Over\")\n",
    "        # We handle the loop length in the experiment section.\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        return state, reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Helper to format the current history as a Tuple (so it's easy to read).\n",
    "        Example Memory-1: ((0, 0),)\n",
    "        \"\"\"\n",
    "        return tuple(self.history)\n",
    "\n",
    "print(\"Environment Class Defined Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd824db1",
   "metadata": {},
   "source": [
    "# Brick 4: Sanity Check\n",
    "Let's play 5 rounds against a **Tit-for-Tat** opponent to see if it works.\n",
    "We will play: C, D, D, C, C.\n",
    "Since the opponent is TFT, they should copy our move from the *previous* turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f86fb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start State: ((0, 0),)\n",
      "I played: Cooperate | New State: ((0, 0),) | Reward: 3\n",
      "I played: Defect | New State: ((1, 0),) | Reward: 5\n",
      "I played: Defect | New State: ((1, 1),) | Reward: 1\n",
      "I played: Cooperate | New State: ((0, 1),) | Reward: 0\n",
      "I played: Cooperate | New State: ((0, 0),) | Reward: 3\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the game environment\n",
    "env = PrisonerDilemmaEnv(opponent_strategy=\"TFT\", memory_length=1)\n",
    "\n",
    "# 2. Reset to start\n",
    "state, _ = env.reset()\n",
    "print(f\"Start State: {state}\")\n",
    "\n",
    "# 3. Play a few manual moves\n",
    "my_moves = [COOPERATE, DEFECT, DEFECT, COOPERATE, COOPERATE]\n",
    "\n",
    "for move in my_moves:\n",
    "    # Take step\n",
    "    state, reward, done, _, _ = env.step(move)\n",
    "\n",
    "    # Translate numbers to words for printing\n",
    "    move_name = \"Cooperate\" if move == 0 else \"Defect\"\n",
    "\n",
    "    print(f\"I played: {move_name} | New State: {state} | Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b640a31",
   "metadata": {},
   "source": [
    "# Brick 5A: Build the MDP Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_states(memory_length):\n",
    "    \"\"\"\n",
    "    Generate all possible states for a given memory length.\n",
    "    Memory-1: 4 states (your_move, opp_move)\n",
    "    Memory-2: 16 states (your_move_t-1, your_move_t-2, opp_move_t-1, opp_move_t-2)\n",
    "    \"\"\"\n",
    "    if memory_length == 1:\n",
    "        # States are (my_last_move, opp_last_move)\n",
    "        states = list(product([COOPERATE, DEFECT], repeat=2))\n",
    "    elif memory_length == 2:\n",
    "        # States are (my_t-1, my_t-2, opp_t-1, opp_t-2)\n",
    "        states = list(product([COOPERATE, DEFECT], repeat=4))\n",
    "    else:\n",
    "        raise ValueError(\"Only memory_length 1 or 2 supported\")\n",
    "\n",
    "    return states\n",
    "\n",
    "\n",
    "def build_transition_and_reward(opponent_strategy, memory_length):\n",
    "    \"\"\"\n",
    "    Build the transition matrix P[s][a][s'] and reward matrix R[s][a].\n",
    "\n",
    "    Returns:\n",
    "        states: list of all states\n",
    "        P: dict P[s][a] = dict of {s': probability}\n",
    "        R: dict R[s][a] = expected immediate reward\n",
    "    \"\"\"\n",
    "    states = get_states(memory_length)\n",
    "    state_set = set(states)\n",
    "    n_states = len(states)\n",
    "\n",
    "    P = {s: {a: {} for a in ACTIONS} for s in states}\n",
    "    R = {s: {a: 0.0 for a in ACTIONS} for s in states}\n",
    "\n",
    "    for s in states:\n",
    "        for a in ACTIONS:  # Agent's action\n",
    "\n",
    "            if memory_length == 1:\n",
    "                my_prev = s[0]  # My previous action (used by TFT)\n",
    "\n",
    "                # Determine opponent's response distribution\n",
    "                if opponent_strategy == \"ALL_C\":\n",
    "                    opp_probs = {COOPERATE: 1.0}\n",
    "\n",
    "                elif opponent_strategy == \"ALL_D\":\n",
    "                    opp_probs = {DEFECT: 1.0}\n",
    "\n",
    "                elif opponent_strategy == \"TFT\":\n",
    "                    # Opponent copies my previous move\n",
    "                    opp_probs = {my_prev: 1.0}\n",
    "\n",
    "                elif opponent_strategy == \"IMPERFECT_TFT\":\n",
    "                    # 90% copy, 10% opposite\n",
    "                    opp_probs = {\n",
    "                        my_prev: 0.9,\n",
    "                        1 - my_prev: 0.1\n",
    "                    }\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy: {opponent_strategy}\")\n",
    "\n",
    "                # Build transitions and expected rewards\n",
    "                for opp_action, prob in opp_probs.items():\n",
    "                    s_next = (a, opp_action)  # New state\n",
    "                    if s_next in state_set:\n",
    "                        P[s][a][s_next] = P[s][a].get(s_next, 0) + prob\n",
    "                        R[s][a] += prob * PAYOFF_MATRIX[(a, opp_action)]\n",
    "\n",
    "            elif memory_length == 2:\n",
    "                # s = (my_t-1, my_t-2, opp_t-1, opp_t-2)\n",
    "                my_t1, my_t2, opp_t1, opp_t2 = s\n",
    "\n",
    "                # Opponent looks at my_t-1 (my most recent move in state)\n",
    "                if opponent_strategy == \"ALL_C\":\n",
    "                    opp_probs = {COOPERATE: 1.0}\n",
    "\n",
    "                elif opponent_strategy == \"ALL_D\":\n",
    "                    opp_probs = {DEFECT: 1.0}\n",
    "\n",
    "                elif opponent_strategy == \"TFT\":\n",
    "                    opp_probs = {my_t1: 1.0}\n",
    "\n",
    "                elif opponent_strategy == \"IMPERFECT_TFT\":\n",
    "                    opp_probs = {\n",
    "                        my_t1: 0.9,\n",
    "                        1 - my_t1: 0.1\n",
    "                    }\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown strategy: {opponent_strategy}\")\n",
    "\n",
    "                # Build transitions: s' = (a, my_t1, opp_new, opp_t1)\n",
    "                for opp_action, prob in opp_probs.items():\n",
    "                    s_next = (a, my_t1, opp_action, opp_t1)\n",
    "                    if s_next in state_set:\n",
    "                        P[s][a][s_next] = P[s][a].get(s_next, 0) + prob\n",
    "                        R[s][a] += prob * PAYOFF_MATRIX[(a, opp_action)]\n",
    "\n",
    "    return states, P, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27586afc",
   "metadata": {},
   "source": [
    "# Brick 5B: Policy Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(states, P, R, policy, gamma, theta=1e-8):\n",
    "    \"\"\"\n",
    "    Evaluate a policy by computing V^π(s) for all states.\n",
    "    Uses iterative method until convergence.\n",
    "\n",
    "    Args:\n",
    "        states: list of states\n",
    "        P: transition probabilities P[s][a][s']\n",
    "        R: rewards R[s][a]\n",
    "        policy: dict mapping state -> action\n",
    "        gamma: discount factor\n",
    "        theta: convergence threshold\n",
    "\n",
    "    Returns:\n",
    "        V: dict mapping state -> value\n",
    "    \"\"\"\n",
    "    # Initialize V arbitrarily\n",
    "    V = {s: 0.0 for s in states}\n",
    "\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in states:\n",
    "            v_old = V[s]\n",
    "            a = policy[s]\n",
    "\n",
    "            # V(s) = R(s,a) + gamma * sum_{s'} P(s'|s,a) * V(s')\n",
    "            v_new = R[s][a]\n",
    "            for s_next, prob in P[s][a].items():\n",
    "                v_new += gamma * prob * V[s_next]\n",
    "\n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "\n",
    "        iteration += 1\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22df90",
   "metadata": {},
   "source": [
    "# Brick 5C: Policy Improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(states, P, R, V, gamma):\n",
    "    \"\"\"\n",
    "    Improve the policy by acting greedily with respect to V.\n",
    "\n",
    "    Returns:\n",
    "        new_policy: dict mapping state -> action\n",
    "        policy_stable: bool, True if policy didn't change\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "\n",
    "    for s in states:\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            # Q(s,a) = R(s,a) + gamma * sum_{s'} P(s'|s,a) * V(s')\n",
    "            q_value = R[s][a]\n",
    "            for s_next, prob in P[s][a].items():\n",
    "                q_value += gamma * prob * V[s_next]\n",
    "\n",
    "            if q_value > best_value:\n",
    "                best_value = q_value\n",
    "                best_action = a\n",
    "\n",
    "        new_policy[s] = best_action\n",
    "\n",
    "    return new_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5ad98",
   "metadata": {},
   "source": [
    "# Brick 5D: Policy Iteration (Main Algorithm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7518c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(opponent_strategy, memory_length, gamma, verbose=True):\n",
    "    \"\"\"\n",
    "    Run Policy Iteration to find the optimal policy.\n",
    "\n",
    "    Args:\n",
    "        opponent_strategy: \"ALL_C\", \"ALL_D\", \"TFT\", or \"IMPERFECT_TFT\"\n",
    "        memory_length: 1 or 2\n",
    "        gamma: discount factor\n",
    "        verbose: print progress\n",
    "\n",
    "    Returns:\n",
    "        optimal_policy: dict mapping state -> action\n",
    "        V: value function\n",
    "        history: list of (policy, V, eval_iterations) per iteration\n",
    "    \"\"\"\n",
    "    # Build MDP\n",
    "    states, P, R = build_transition_and_reward(opponent_strategy, memory_length)\n",
    "\n",
    "    # Initialize policy (start with all cooperate)\n",
    "    policy = {s: COOPERATE for s in states}\n",
    "\n",
    "    history = []\n",
    "    iteration = 0\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        V, eval_iters = policy_evaluation(states, P, R, policy, gamma)\n",
    "\n",
    "        # Policy Improvement\n",
    "        new_policy = policy_improvement(states, P, R, V, gamma)\n",
    "\n",
    "        # Check for convergence\n",
    "        policy_stable = all(policy[s] == new_policy[s] for s in states)\n",
    "\n",
    "        history.append({\n",
    "            'iteration': iteration,\n",
    "            'policy': policy.copy(),\n",
    "            'V': V.copy(),\n",
    "            'eval_iterations': eval_iters\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Iteration {iteration}: Policy Evaluation took {eval_iters} iterations\")\n",
    "            print(f\"  Policy: {format_policy(policy, memory_length)}\")\n",
    "\n",
    "        if policy_stable:\n",
    "            if verbose:\n",
    "                print(f\"\\n✓ Policy converged after {iteration + 1} iterations!\")\n",
    "            break\n",
    "\n",
    "        policy = new_policy\n",
    "        iteration += 1\n",
    "\n",
    "        # Safety limit\n",
    "        if iteration > 100:\n",
    "            print(\"Warning: Max iterations reached\")\n",
    "            break\n",
    "\n",
    "    return policy, V, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a145d39d",
   "metadata": {},
   "source": [
    "# Brick 5E: Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_policy(policy, memory_length):\n",
    "    \"\"\"Pretty print the policy.\"\"\"\n",
    "    action_names = {COOPERATE: 'C', DEFECT: 'D'}\n",
    "\n",
    "    if memory_length == 1:\n",
    "        result = {}\n",
    "        for s, a in policy.items():\n",
    "            s_str = f\"({action_names[s[0]]},{action_names[s[1]]})\"\n",
    "            result[s_str] = action_names[a]\n",
    "        return result\n",
    "    else:\n",
    "        # For Memory-2, just show a summary\n",
    "        return {str(s): action_names[a] for s, a in policy.items()}\n",
    "\n",
    "\n",
    "def format_state(s, memory_length):\n",
    "    \"\"\"Format a state tuple nicely.\"\"\"\n",
    "    names = {COOPERATE: 'C', DEFECT: 'D'}\n",
    "    return tuple(names[x] for x in s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ee40a",
   "metadata": {},
   "source": [
    "# Brick 6: Simulation Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004dda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy(opponent_strategy, memory_length, policy, n_episodes=50, n_steps=50, seed=42):\n",
    "    \"\"\"\n",
    "    Simulate the optimal policy against the opponent.\n",
    "\n",
    "    Returns:\n",
    "        avg_cumulative_reward: average total reward per episode\n",
    "        all_rewards: list of episode rewards\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    all_rewards = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        # Initialize state (all cooperate history)\n",
    "        if memory_length == 1:\n",
    "            state = (COOPERATE, COOPERATE)\n",
    "            history = [(COOPERATE, COOPERATE)]\n",
    "        else:\n",
    "            state = (COOPERATE, COOPERATE, COOPERATE, COOPERATE)\n",
    "            history = [(COOPERATE, COOPERATE), (COOPERATE, COOPERATE)]\n",
    "\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            # Agent acts according to policy\n",
    "            action = policy[state]\n",
    "\n",
    "            # Opponent responds\n",
    "            if memory_length == 1:\n",
    "                my_prev = state[0]\n",
    "            else:\n",
    "                my_prev = state[0]  # my_t-1\n",
    "\n",
    "            if opponent_strategy == \"ALL_C\":\n",
    "                opp_action = COOPERATE\n",
    "            elif opponent_strategy == \"ALL_D\":\n",
    "                opp_action = DEFECT\n",
    "            elif opponent_strategy == \"TFT\":\n",
    "                opp_action = my_prev\n",
    "            elif opponent_strategy == \"IMPERFECT_TFT\":\n",
    "                if np.random.random() < 0.9:\n",
    "                    opp_action = my_prev\n",
    "                else:\n",
    "                    opp_action = 1 - my_prev\n",
    "\n",
    "            # Get reward\n",
    "            reward = PAYOFF_MATRIX[(action, opp_action)]\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update state\n",
    "            if memory_length == 1:\n",
    "                state = (action, opp_action)\n",
    "            else:\n",
    "                state = (action, state[0], opp_action, state[2])\n",
    "\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "    return np.mean(all_rewards), all_rewards\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7bd72",
   "metadata": {},
   "source": [
    "# Main: Run All Experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbb624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "POLICY ITERATION EXPERIMENTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MEMORY-1\n",
      "============================================================\n",
      "\n",
      "--- Opponent: ALL_C ---\n",
      "\n",
      "γ = 0.1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ACTIONS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mγ = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgamma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Run Policy Iteration\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m policy, V, history \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopponent_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Print optimal policy\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Optimal Policy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_policy(policy,\u001b[38;5;250m \u001b[39mmemory)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[1;34m(opponent_strategy, memory_length, gamma, verbose)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mRun Policy Iteration to find the optimal policy.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    history: list of (policy, V, eval_iterations) per iteration\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Build MDP\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m states, P, R \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_transition_and_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopponent_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize policy (start with all cooperate)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m policy \u001b[38;5;241m=\u001b[39m {s: COOPERATE \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states}\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36mbuild_transition_and_reward\u001b[1;34m(opponent_strategy, memory_length)\u001b[0m\n\u001b[0;32m     29\u001b[0m state_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(states)\n\u001b[0;32m     30\u001b[0m n_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(states)\n\u001b[1;32m---> 32\u001b[0m P \u001b[38;5;241m=\u001b[39m {s: {a: {} \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ACTIONS} \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states}\n\u001b[0;32m     33\u001b[0m R \u001b[38;5;241m=\u001b[39m {s: {a: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ACTIONS} \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states}\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states:\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m state_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(states)\n\u001b[0;32m     30\u001b[0m n_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(states)\n\u001b[1;32m---> 32\u001b[0m P \u001b[38;5;241m=\u001b[39m {s: {a: {} \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[43mACTIONS\u001b[49m} \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states}\n\u001b[0;32m     33\u001b[0m R \u001b[38;5;241m=\u001b[39m {s: {a: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m ACTIONS} \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states}\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ACTIONS' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    opponents = [\"ALL_C\", \"ALL_D\", \"TFT\", \"IMPERFECT_TFT\"]\n",
    "    gammas = [0.1, 0.5, 0.9, 0.99]\n",
    "    memory_lengths = [1, 2]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"POLICY ITERATION EXPERIMENTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for memory in memory_lengths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MEMORY-{memory}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        for opponent in opponents:\n",
    "            print(f\"\\n--- Opponent: {opponent} ---\")\n",
    "\n",
    "            for gamma in gammas:\n",
    "                print(f\"\\nγ = {gamma}\")\n",
    "\n",
    "                # Run Policy Iteration\n",
    "                policy, V, history = policy_iteration(\n",
    "                    opponent_strategy=opponent,\n",
    "                    memory_length=memory,\n",
    "                    gamma=gamma,\n",
    "                    verbose=False\n",
    "                )\n",
    "\n",
    "                # Print optimal policy\n",
    "                print(f\"  Optimal Policy: {format_policy(policy, memory)}\")\n",
    "\n",
    "                # Simulate to verify\n",
    "                avg_reward, _ = simulate_policy(\n",
    "                    opponent_strategy=opponent,\n",
    "                    memory_length=memory,\n",
    "                    policy=policy,\n",
    "                    n_episodes=50,\n",
    "                    n_steps=50\n",
    "                )\n",
    "                print(f\"  Avg Cumulative Reward (50 episodes × 50 steps): {avg_reward:.2f}\")\n",
    "\n",
    "                # Store results\n",
    "                key = (memory, opponent, gamma)\n",
    "                results[key] = {\n",
    "                    'policy': policy,\n",
    "                    'V': V,\n",
    "                    'iterations': len(history),\n",
    "                    'avg_reward': avg_reward\n",
    "                }\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUMMARY TABLE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Print summary\n",
    "    for memory in memory_lengths:\n",
    "        print(f\"\\n=== Memory-{memory} ===\")\n",
    "        print(f\"{'Opponent':<15} {'γ=0.1':<12} {'γ=0.5':<12} {'γ=0.9':<12} {'γ=0.99':<12}\")\n",
    "        print(\"-\" * 55)\n",
    "\n",
    "        for opponent in opponents:\n",
    "            row = f\"{opponent:<15}\"\n",
    "            for gamma in gammas:\n",
    "                key = (memory, opponent, gamma)\n",
    "                policy = results[key]['policy']\n",
    "                # Count how many states have Defect\n",
    "                defects = sum(1 for a in policy.values() if a == DEFECT)\n",
    "                total = len(policy)\n",
    "                row += f\" {defects}/{total} D\".ljust(12)\n",
    "            print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202a172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
