\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{parskip}

\begin{document}

\section*{Part II: Define the MDP}

In this section, we formally define the Markov Decision Process (MDP) for the Repeated Prisoners Dilemma for each observation scheme (Memory-1 and Memory-2) and for each opponent strategy. The MDP is defined by the tuple:
\[
(\mathcal{S}, \mathcal{A}, P, R, \gamma),
\]
where $\mathcal{S}$ is the state space, $\mathcal{A}$ the action space, $P$ the transition probabilities, $R$ the reward function, and $\gamma \in (0,1)$ the discount factor.

The agent’s actions are always:
\[
\mathcal{A} = \{ C, D \}.
\]

The opponent strategies considered are:
\begin{itemize}
    \item \textbf{ALL-C}: always cooperates.
    \item \textbf{ALL-D}: always defects.
    \item \textbf{TFT (Tit-for-Tat)}: starts with $C$, then copies the agent’s previous action.
    \item \textbf{Imperfect TFT}: with probability $0.9$ copies the agent’s previous action, and with probability $0.1$ plays the opposite.
\end{itemize}

We now define the state space, transition dynamics, and reward function for each observation scheme.

\subsection{Memory-1 Observation Scheme}

\subsubsection*{State Space}
In Memory-1 the agent observes the actions of the previous round:
\[
s = (\text{your previous move},\ \text{opponent previous move}) \in \{C,D\}^2.
\]
Thus, the state space contains four states:
\[
\mathcal{S}_{M1} = \{ (C,C), (C,D), (D,C), (D,D) \}.
\]
The initial state is defined as $(C,C)$.

\subsubsection*{Transition Probabilities $P(s'|s,a)$}

Let $s=(a_{\text{you}}^{t-1}, a_{\text{opp}}^{t-1})$ and let $a$ be your current action. The next state is
\[
s' = (a,\ a_{\text{opp}}^{t}).
\]

The transition depends entirely on the opponent strategy:

\paragraph{ALL-C.}
The opponent always plays $C$, hence:
\[
P\big( (a, C) \mid (x,y), a \big) = 1.
\]

\paragraph{ALL-D.}
The opponent always plays $D$, hence:
\[
P\big( (a, D) \mid (x,y), a \big) = 1.
\]

\paragraph{Tit-for-Tat.}
Opponent plays your previous action:
\[
a_{\text{opp}}^{t} = a_{\text{you}}^{t-1}.
\]
Hence:
\[
P\big( (a, a_{\text{you}}^{t-1}) \mid (a_{\text{you}}^{t-1}, a_{\text{opp}}^{t-1}), a \big) = 1.
\]

\paragraph{Imperfect Tit-for-Tat.}
Opponent copies your previous action with probability $0.9$ and does the opposite with probability $0.1$. Let $a_{\text{you}}^{t-1}$ be your previous action. Define $\neg C = D$ and $\neg D = C$. Then:
\[
P\big( (a, a_{\text{you}}^{t-1}) \mid s, a \big) = 0.9,
\qquad
P\big( (a, \neg a_{\text{you}}^{t-1}) \mid s, a \big) = 0.1.
\]

\subsubsection*{Reward Function $R(s,a)$}

The reward is determined by your action $a$ and the opponent’s next action $a_{\text{opp}}^{t}$ using the Prisoner’s Dilemma payoff matrix:
\[
R = 3,\ S = 0,\ T = 5,\ P = 1.
\]

For deterministic opponents (ALL-C, ALL-D, TFT):
\[
R(s,a) =
\begin{cases}
3 & a=C,\ a_{\text{opp}}^{t}=C,\\
0 & a=C,\ a_{\text{opp}}^{t}=D,\\
5 & a=D,\ a_{\text{opp}}^{t}=C,\\
1 & a=D,\ a_{\text{opp}}^{t}=D.
\end{cases}
\]

For Imperfect TFT, the reward is the expectation over the opponent's stochastic action:
\[
\mathbb{E}[R(s,a)] = 0.9\, R(a, a_{\text{you}}^{t-1}) + 0.1\, R(a, \neg a_{\text{you}}^{t-1}).
\]

\subsection{Memory-2 Observation Scheme}

\subsubsection*{State Space}

In Memory-2 the agent observes the last two actions from both players:
\[
s = (a^{t-1}_{\text{you}}, a^{t-2}_{\text{you}}, a^{t-1}_{\text{opp}}, a^{t-2}_{\text{opp}}) \in \{C,D\}^4.
\]
Thus the state space contains:
\[
|\mathcal{S}_{M2}| = 2^4 = 16 \text{ states.}
\]
The initial state is $(C,C,C,C)$.

\subsubsection*{Transition Probabilities}

At time $t$, after taking action $a$, the updated state shifts the history:
\[
s' = (a,\ a^{t-1}_{\text{you}},\ a_{\text{opp}}^{t},\ a^{t-1}_{\text{opp}}).
\]

Opponent dynamics follow the same rules as in Memory-1 (ALL-C, ALL-D, TFT, Imperfect TFT), but the history shift increases the number of possible $s'$.

Explicitly:

\paragraph{ALL-C.}
\[
P(s' \mid s,a) = 1 \quad \text{with } s' = (a,\ a^{t-1}_{\text{you}},\ C,\ a^{t-1}_{\text{opp}}).
\]

\paragraph{ALL-D.}
\[
P(s' \mid s,a) = 1 \quad \text{with } s' = (a,\ a^{t-1}_{\text{you}},\ D,\ a^{t-1}_{\text{opp}}).
\]

\paragraph{TFT.}
Opponent plays your action from one step earlier $a^{t-1}_{\text{you}}$:
\[
P(s' \mid s,a) = 1 \quad \text{with } s' = (a,\ a^{t-1}_{\text{you}},\ a^{t-1}_{\text{you}},\ a^{t-1}_{\text{opp}}).
\]

\paragraph{Imperfect TFT.}
Opponent plays:
\[
a_{\text{opp}}^{t} =
\begin{cases}
a^{t-1}_{\text{you}} & \text{with prob } 0.9,\\
\neg a^{t-1}_{\text{you}} & \text{with prob } 0.1.
\end{cases}
\]
Thus:
\[
P(s' \mid s,a) =
\begin{cases}
0.9 & \text{if } s' = (a,\ a^{t-1}_{\text{you}},\ a^{t-1}_{\text{you}},\ a^{t-1}_{\text{opp}}),\\[4pt]
0.1 & \text{if } s' = (a,\ a^{t-1}_{\text{you}},\neg a^{t-1}_{\text{you}},\ a^{t-1}_{\text{opp}}).\\
\end{cases}
\]

\subsubsection*{Reward Function}

The reward in Memory-2 is identical to Memory-1:
\[
R(s,a) = R(a,\ a_{\text{opp}}^{t}),
\]
with expected reward for Imperfect TFT computed as:
\[
\mathbb{E}[R(s,a)] = 0.9\, R(a, a^{t-1}_{\text{you}}) + 0.1\, R(a, \neg a^{t-1}_{\text{you}}).
\]

\bigskip

\noindent
This completes the formal MDP definition for both observation schemes and all opponent types.

\end{document}